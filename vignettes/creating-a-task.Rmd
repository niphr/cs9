---
title: "Creating a task"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Creating a task}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Tutorial 1: Introduction



Implementing cs9 requires a number of functions to be called in the correct order. To make this as simple as possible, we have provided a skeleton implementation at https://github.com/csids/cs9example/

We suggest that you clone this GitHub repo to your server, and then do a global find/replace on `cs9example` with the name you want for your R package.

For the purposes of this vignette, we assume that the reader is either using RStudio Server Open Source or RStudio Workbench inside Docker containers that have been built according to the cs9 specifications. We will refer to your implementation of RStudio Server Open Source/RStudio Workbench with the generic term "RStudio".

## Load the code

Open`cs9example` in [RStudio project mode](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects). Restart the R session via `Ctrl+Shift+F10`, `rstudioapi::restartSession()`, or `Session > Restart R`. This will ensure that you have a clean working environment before you begin. You may now load your sc implementation. This can be done via `Ctrl+Shift+L`, `devtools::load_all(".")`, or `Build > Load All`.

<aside>
  In general, we recommend cleaning your working environment every time before running `devtools::load_all(".")`.
</aside>


``` r
rstudioapi::restartSession()
devtools::load_all(".")
```

You can now see which tables have been loaded by examining the surveillance system object. These tables were included in the skeleton. Note that tables beginning with `config_*` are special tables that are automatically generated by CS9.


``` r
names(global$ss$tables)
Error: object 'global' not found
```

When you do this you will not see the schemas related to weather, income, and houseprices and you might see a Warning on the form of "In setup_ns_exports(path, export_all, export_imports) : Objects listed as exports.....". This is as expected.

You can also see which tasks have been loaded by examining the tasks in the surveillance system. These tasks were included in the skeleton. We have not yet made any tasks, hence you will see an empty list.


``` r
names(global$ss$tasks)
Error: object 'global' not found
```

<!-- ## Running -->

  <!-- You can now run these tasks in your console if you want. Note that we use `scskeleton::tm_run_task` instead of `global$ss$run_task`. This is because we want to ensure that `scexample::.onLoad` has been called which authenticates you. -->

  <!-- ```{r eval=FALSE, include=TRUE} -->
  <!-- scskeleton::tm_run_task("weather_download_and_import_rawdata") -->
  <!-- scskeleton::tm_run_task("weather_clean_data") -->
  <!-- scskeleton::tm_run_task("weather_export_weather_plots") -->
  <!-- ``` -->


  <!-- The rest of the tutorial will walk you through how to create these three tasks. -->

  ## Weather data example

  We are now going to create a weather data example. Our end goal is to plot the minimum and maximal temperature of all counties in Norway. This involves a task for downloading and importing raw data. For this we need to specify a schema which describes the data we want to store, which data identifies unique rows and who has access to the data. We also need to define the task through a task definition, i.e., task name, how many cores we want to use, the structure of the task, common arguments etc. Finally we actually implement the task by writing a data selector function, an action function and sometimes a more detailed function describing the plans and analyses of the task.

We also create a task for cleaning the raw data, again with a schema, a task definition and an implementation of a data selector function and an action function.

Finally we create a task for the creation of the plots.

All the schemas are spesified in `03_db_schemas.r`, all the task definitions are specified in `04_tasks.r`. The data selector functions and the action functions corresponding to each task have there own respective script with the name specified in the task description.

## Developing weather_download_and_import_rawdata

We will walk you through the development of a task that downloads weather data from an API and imports the raw data into a database table.

### 1. Schemas

The first step when developing any task is specifying the schemas that will be used.

It is strongly recommended that you use the RStudio `Addins` menu to help you quickly insert code templates.


```
Error in knitr::include_graphics("rmd/db-schemas/addins.png"): Cannot find the file(s): "rmd/db-schemas/addins.png"
```

If you go into the script `03_db_schemas.r` you can see a function called `set_db_schemas`. All schemas are placed within this function. If you scroll down you can see that there is already a schema called `anon_example_weather_rawdata` which is commented out.


```
Warning in file(con, "r"): cannot open URL 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/03_db_schemas.r': HTTP status
was '404 Not Found'
Error in file(con, "r"): cannot open the connection to 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/03_db_schemas.r'
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L18-L64

18 |     argset <- global$ss$shortcut_get_argset("weather_download_and_import_rawdata", index_plan = index_plan, index_analysis = index_analysis)
19 |     tables <- global$ss$shortcut_get_tables("weather_download_and_import_rawdata")
20 |   }
21 | 
22 |   # special case that runs before everything
23 |   if (argset$first_analysis == TRUE) {
24 | 
25 |   }
26 | 
27 |   a <- data$data$properties$timeseries
28 |   res <- vector("list", length=length(a) - 1)
29 |   for(i in seq_along(res)){
30 |     # i = 1
31 |     time_from <- a[[i]]$time
32 |     if("next_1_hours" %in% names(a[[i]]$data)){
33 |       time_var <- "next_1_hours"
34 |     } else {
35 |       time_var <- "next_6_hours"
36 |     }
37 |     temp <- a[[i]]$data[["instant"]]$details$air_temperature
38 |     precip <- a[[i]]$data[[time_var]]$details$precipitation_amount
39 | 
40 |     res[[i]] <- data.frame(
41 |       time_from = as.character(time_from),
42 |       temp = as.numeric(temp),
43 |       precip = as.numeric(precip)
44 |     )
45 |   }
46 | 
47 |   res <- rbindlist(res)
48 |   res <- res[stringr::str_sub(time_from, 12, 13) %in% c("00", "06", "12", "18")]
49 |   res[, date := as.Date(stringr::str_sub(time_from, 1, 10))]
50 |   res <- res[
51 |     ,
52 |     .(
53 |       temp_max = max(temp),
54 |       temp_min = min(temp),
55 |       precip = sum(precip)
56 |     ),
57 |     keyby = .(date)
58 |   ]
59 | 
60 |   # we look at the downloaded data
61 |   # res
62 | 
63 |   # we now need to format it
64 |   res[, granularity_time := "date"]
```

We are now going to recreate this schema.
Make sure your pointer is inside of the curly brackets. Go to the `Addins` menu and click `Insert db schema (anon)`. You have now created a boiler plate for your schema.

#### Schema name

Start by replacing `GROUPING_VARIANT` in `anon_GROUPING_VARIANT` with the name of your schema. For example `example_weather_rawdata`. The grouping will now be `example_weather` and the variant is `rawdata`.



```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L20-L22

20 |   }
21 | 
22 |   # special case that runs before everything
```
Fill this inn for `name_grouping` and `name_variant`. The name of the schema is then `anon_example_weather_rawdata`.

In the example we define the name of the schema to be `anon_example_weather_weather_rawdata`.

#### Validators

The validators are pre-made and you do not have to change anything.


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L61-L63

61 |   # res
62 | 
63 |   # we now need to format it
```

These are validators that check:

  - Are the column names/field types in the schema definition in line with style guidelines?
  - Are the values/field contents of the datasets that will be uploaded to the database correct? E.g. Does a date column actually contain dates?

  When using `validator_field_types = csdb::validator_field_types_csfmt_rts_data_v1` we expect that the first 16 columns are always as follows (i.e. standardized structural data):


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L25-L43

25 |   }
26 | 
27 |   a <- data$data$properties$timeseries
28 |   res <- vector("list", length=length(a) - 1)
29 |   for(i in seq_along(res)){
30 |     # i = 1
31 |     time_from <- a[[i]]$time
32 |     if("next_1_hours" %in% names(a[[i]]$data)){
33 |       time_var <- "next_1_hours"
34 |     } else {
35 |       time_var <- "next_6_hours"
36 |     }
37 |     temp <- a[[i]]$data[["instant"]]$details$air_temperature
38 |     precip <- a[[i]]$data[[time_var]]$details$precipitation_amount
39 | 
40 |     res[[i]] <- data.frame(
41 |       time_from = as.character(time_from),
42 |       temp = as.numeric(temp),
43 |       precip = as.numeric(precip)
```

The field `info` should contain a short description of the data table.

#### Field types/column names

Add the spesific column names and types needed. In our case we want to store the maximum and minimum temperature and the precipitation. Call them "temp_max", "temp_min", and "precip". These are all "DOUBLE". Remove `"XXXX_n" = "INTEGER"`, and `"XXXX_pr" = "DOUBLE"`as these are dummy variables.


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L45-L47

45 |   }
46 | 
47 |   res <- rbindlist(res)
```

These are the extra columns that contain the context-specific data in this dataset.

#### Keys

The combination of these columns represents a unique row in the dataset. In this dataset the combination of "granularity_geo", "location_code", "date", "age", "sex" which are the initial suggestions are sufficient.



```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L49-L55

49 |   res[, date := as.Date(stringr::str_sub(time_from, 1, 10))]
50 |   res <- res[
51 |     ,
52 |     .(
53 |       temp_max = max(temp),
54 |       temp_min = min(temp),
55 |       precip = sum(precip)
```



#### Censoring


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L56-L60

56 |     ),
57 |     keyby = .(date)
58 |   ]
59 | 
60 |   # we look at the downloaded data
```

Censoring that is applied to the datasets. In this example we do not apply censoring hence remove the boiler plate suggestions.

### 2. Task definition (task_from_config)

Now we have a schema. The second step is defining the task.

Again it is strongly recommended that you use the RStudio `Addins` menu to help you quickly insert code templates.

Go to script `04_tasks.r` and place your curser inside of the curly bracket. Use the addins menu and click `Insert task_from_config`.


```
Error in knitr::include_graphics("rmd/tasks/addins_2.png"): Cannot find the file(s): "rmd/tasks/addins_2.png"
```

Now you have a boilerplate for a task definition.



```
Warning in file(con, "r"): cannot open URL 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/04_tasks.r': HTTP status was
'404 Not Found'
Error in file(con, "r"): cannot open the connection to 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/04_tasks.r'
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L21-L43

21 | 
22 |   # special case that runs before everything
23 |   if (argset$first_analysis == TRUE) {
24 | 
25 |   }
26 | 
27 |   a <- data$data$properties$timeseries
28 |   res <- vector("list", length=length(a) - 1)
29 |   for(i in seq_along(res)){
30 |     # i = 1
31 |     time_from <- a[[i]]$time
32 |     if("next_1_hours" %in% names(a[[i]]$data)){
33 |       time_var <- "next_1_hours"
34 |     } else {
35 |       time_var <- "next_6_hours"
36 |     }
37 |     temp <- a[[i]]$data[["instant"]]$details$air_temperature
38 |     precip <- a[[i]]$data[[time_var]]$details$precipitation_amount
39 | 
40 |     res[[i]] <- data.frame(
41 |       time_from = as.character(time_from),
42 |       temp = as.numeric(temp),
43 |       precip = as.numeric(precip)
```

#### Task name

Replace `TASK_NAME` by your task name. For example `weather_download_and_import_rawdata`. `weather` is the task/grouping and `download_and_import_rawdata`is the action name. Insert these for `name_grouping`, and `name_action`. For `name_variant` use `NULL`.



```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L22-L24

22 |   # special case that runs before everything
23 |   if (argset$first_analysis == TRUE) {
24 | 
```

Now the name of the task is defined to be `weather_download_and_import_rawdata`.

#### CPU cores


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L25-L25

25 |   }
```

We specify that the plans will run sequentially with 1 CPU core. If the number of CPU cores is 2 or higher then the first and last plans will run sequentially, and all the plans in the middle will run in parallel. The first and last plans always run sequentially because this allows us to write "special" code for the first and last plans (i.e. "do this before everything runs" and "do this after everything runs").

#### Plan/analysis structure


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L26-L30

26 | 
27 |   a <- data$data$properties$timeseries
28 |   res <- vector("list", length=length(a) - 1)
29 |   for(i in seq_along(res)){
30 |     # i = 1
```

We specify the plan/analysis structure here. You may use one of the following combinations:

  - `plan_analysis_fn_name` (rarely used)
- `for_each_plan` (plan-heavy, one analysis per plan)
- `for_each_plan` + `for_each_analysis` (typically analysis-heavy)

`plan_analysis_fn_name` is a (rarely used) function that will provide a list containing the plan/analysis structure. It is generally only used when the plan/analysis structure needs to be reactive depending upon some external data (e.g. "an unknown number of data files are provided each day and need to be cleaned").

`for_each_plan` is a list, with each element corresponding to a plan defined by a named list. Within this named list, each of the named elements will be translated into argset elements that are available for the respective plans. This particular `for_each_plan` defines a task with 356 plans (one for each municipality).

`for_each_analysis` is nearly the same as `for_each_plan`. It specifies what kind of analyses you would like to perform within each plan. It is a named list, with each element corresponding to an analysis defined by a named list. Within this named list, each of the named elements will be translated into argset elements that are available for the respective analyses.

An example of a `for_each_plan` that would correspond to 11 tasks (one for each county):


``` r
options(width = 150)
for_each_plan = plnr::expand_list(
  location_code = fhidata::norway_locations_names()[granularity_geo %in% c("county")]$location_code
)
for_each_plan
[[1]]
[[1]]$location_code
[1] "county42"


[[2]]
[[2]]$location_code
[1] "county34"


[[3]]
[[3]]$location_code
[1] "county15"


[[4]]
[[4]]$location_code
[1] "county18"


[[5]]
[[5]]$location_code
[1] "county03"


[[6]]
[[6]]$location_code
[1] "county11"


[[7]]
[[7]]$location_code
[1] "county54"


[[8]]
[[8]]$location_code
[1] "county50"


[[9]]
[[9]]$location_code
[1] "county38"


[[10]]
[[10]]$location_code
[1] "county46"


[[11]]
[[11]]$location_code
[1] "county30"
```

`fhidata::norway_locations_names()` gives us location codes in Norway (try and run if in your console). Implement a plan in your task which has the location codes off all municipalities (municip) in Norway.


### Universal argset


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L31-L31

31 |     time_from <- a[[i]]$time
```

Here we can specify a named list, where each of the named elements will be translated into argset elements that are available for all plans/analyses.

#### Upsert/insert at end of each plan


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L32-L33

32 |     if("next_1_hours" %in% names(a[[i]]$data)){
33 |       time_var <- "next_1_hours"
```

If you include a schema called `output`, then these options will let you upsert/insert the returned value from `action_fn_name` at the end of each plan. This is an important nuance, because when you write/develop your task, you can (typically) only write one function (`action_fn_name`) that is applied to all analyses. This means that if your `action_fn` wants to upsert/insert data to a schema, it (typically) will do this within *every* analysis. If you have an analysis-heavy task, then this will be a lot of frequent traffic to the databases, which may affect performance. By using these flags, you can restrict the upsert/insert to the end of the plan, which may increase performance.

#### action_fn_name

The action_fn_name specifies the name of the function that corresponds to the action. That is, the function that is called in every analysis. Note that:

  - This is a string
- It must include the package name
- It is typically of the form `PACKAGE::TASK_action`

In our case the package is `scskeleton` and the TASK is `weather_download_and_import_rawdata`. Insert this in your task.


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L34-L34

34 |     } else {
```


#### data_selector_fn_name

The data_selecotr_fn_name specifies the name of the function that corresponds to the data selector. That is, the function that is called at the start of every plan to provide data to all of the analyses inside the plan. Note that:

  - This is a string
- It must include the package name
- It is typically of the form `PACKAGE::TASK_data_selector`

Try and guess what this would be in our example.


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L35-L35

35 |       time_var <- "next_6_hours"
```


#### Schemas

The schemas specify a named list, where each element consists of a schema. The names will be passed through as `schema$name` in `action_fn_name` and `data_selector_fn_name`. We must include both the schemas where we get data from and the schemas we store data to.

In our example we do not yet have data so we only specify the schema we have earlier which we called `anon_example_weather_rawdata`. This meand you can remove the boiler plate input schema and replace `SCHEMA_NAME_2` with `anon_example_weather_rawdata`.


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L36-L41

36 |     }
37 |     temp <- a[[i]]$data[["instant"]]$details$air_temperature
38 |     precip <- a[[i]]$data[[time_var]]$details$precipitation_amount
39 | 
40 |     res[[i]] <- data.frame(
41 |       time_from = as.character(time_from),
```

#### Task description

Finally create a small task description.

### 3. data_selector_fn

The third step in creating a task is defining a data selector function. This is the function that will perform the "one data-pull per plan" and subsequently provide the data to the action.

Go to script `weather_download_and_import_rawdata.r`.

Use the RStudio `Addins` menu to help you quickly insert code templates by clicking `Insert action and data selector`.


```
Error in knitr::include_graphics("rmd/tasks/addins_3.png"): Cannot find the file(s): "rmd/tasks/addins_3.png"
```


Just like that, a pre-made boilerplate is ready to go! Find the data_selector part of the script and replace `TASK_NAME` with our task name `weather_download_and_import_rawdata`.




#### plnr::is_run_directly()


```
Warning in file(con, "r"): cannot open URL
'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/weather_download_and_import_rawdata.r': HTTP status was '404 Not Found'
Error in file(con, "r"): cannot open the connection to 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/weather_download_and_import_rawdata.r'
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L94-L101

 94 |     index_plan <- 1
 95 | 
 96 |     argset <- global$ss$shortcut_get_argset("weather_download_and_import_rawdata", index_plan = index_plan)
 97 |     tables <- global$ss$shortcut_get_tables("weather_download_and_import_rawdata")
 98 |   }
 99 | 
100 |   # find the mid lat/long for the specified location_code
101 |   gps <- csmaps::nor_municip_map_b2024_default_dt[location_code == argset$location_code,.(
```

At the top of all `data_selector_fn`s you will see a section of code wrapped inside `if (plnr::is_run_directly()) {`. This code will **only** be run if it is manually highlighted inside RStudio and then "run". This is extremely beneficial to the user, because it means that the user can easily write small pieces of code that are only used during development, which will not be run when the code is run "properly".

Sykdomspulsen core uses these sections to let the user "jump" directly into the function. Look at the arguments for `weather_download_and_import_rawdata_data_selector` and you will see that it needs `argset` and `schema`.

The code inside `if (plnr::is_run_directly()) {` loads `argset` and `schema` for `index_plan = 1`. **By running these lines, you can treat the inside of `weather_download_and_import_rawdata_data_selector` as an interactive script!**

  This makes the development of the code extremely easy as "everything is an interactive script".

Check that you have an argset and a schema by running the lines within `if (plnr::is_run_directly()) {}`.

### Getting data

The majority of the `data_selector_fn` is concerned with selecting data (obviously). Remember that the data should be selected to meet the needs of the plan. If you have 11 plans (one for each county), then your `data_selector_fn` should only extract data for the county of interest.


Take a look at your argset for plan = 1.
Since we do not have input data from a schema we can remove the premade schema. Instead we are going to get data from `fhimaps::norway_lau2_map_b2020_default_dt` which provides latitudes (lat) and longitudes (long). Explore the available data by running `fhimaps::norway_lau2_map_b2020_default_dt` in your console. It returns a data table. We only want the mean latitude and longitude of the specific location_code for this particular plan and analysis. Therefor try and select only this data and call it `gps`.

Now we download the weather forcast for this specific location from an api by using httr::GET and glue::glue to get the right address.

httr::GET(glue::glue("https://api.met.no/weatherapi/locationforecast/2.0/classic?lat={gps$lat}&lon={gps$long}"), httr::content_type_xml()).
Use xml2::read_xml() to read the content. Take a peak below and implement it yourself.


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L103-L111

103 |     long = mean(long)
104 |   )]
105 | 
106 |   # download the forecast for the specified location_code
107 |   d <- httr::GET(glue::glue("https://api.met.no/weatherapi/locationforecast/2.0/complete?lat={gps$lat}&lon={gps$long}"))
108 |   d <- httr::content(d)
109 | 
110 |   # The variable returned must be a named list
111 |   retval <- list(
```


### Returning data

`data_selector_fn` needs to return a named list. This will be made available to the user in `action_fn` (`weather_download_and_import_rawdata_action`) via the argument `data`.

In your task replace "NAME" by the name for your data for example "data".


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L113-L116

113 |   )
114 | 
115 |   retval
116 | }
```

The entire data selector function should now look like this.


```
Warning in file(con, "r"): cannot open URL
'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/weather_download_and_import_rawdata.r': HTTP status was '404 Not Found'
Error in file(con, "r"): cannot open the connection to 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/weather_download_and_import_rawdata.r'
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L88-L119

 88 | #' @param tables DB tables
 89 | #' @export
 90 | weather_download_and_import_rawdata_data_selector <- function(argset, tables) {
 91 |   if (plnr::is_run_directly()) {
 92 |     # sc::tm_get_plans_argsets_as_dt("weather_download_and_import_rawdata")
 93 | 
 94 |     index_plan <- 1
 95 | 
 96 |     argset <- global$ss$shortcut_get_argset("weather_download_and_import_rawdata", index_plan = index_plan)
 97 |     tables <- global$ss$shortcut_get_tables("weather_download_and_import_rawdata")
 98 |   }
 99 | 
100 |   # find the mid lat/long for the specified location_code
101 |   gps <- csmaps::nor_municip_map_b2024_default_dt[location_code == argset$location_code,.(
102 |     lat = mean(lat),
103 |     long = mean(long)
104 |   )]
105 | 
106 |   # download the forecast for the specified location_code
107 |   d <- httr::GET(glue::glue("https://api.met.no/weatherapi/locationforecast/2.0/complete?lat={gps$lat}&lon={gps$long}"))
108 |   d <- httr::content(d)
109 | 
110 |   # The variable returned must be a named list
111 |   retval <- list(
112 |     "data" = d
113 |   )
114 | 
115 |   retval
116 | }
117 | 
118 | # **** functions **** ----
119 | NA
```



Check that the data selector function works by restarting R (`ctrl + shift + F10`) and loading the packages (`ctrl + shift + L`) before running through the data selector function line by line.

### 4. action_fn

The fourth step is defining an action function. This is the function that will perform the "action" within the the analysis. That is, given:

  - data
- argset
- schema

What do you actually want to *do* with them? Find the action part in your script and replace  `TASK_NAME` with our task name `weather_download_and_import_rawdata`.

#### plnr::is_run_directly()


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L95-L102

 95 | 
 96 |     argset <- global$ss$shortcut_get_argset("weather_download_and_import_rawdata", index_plan = index_plan)
 97 |     tables <- global$ss$shortcut_get_tables("weather_download_and_import_rawdata")
 98 |   }
 99 | 
100 |   # find the mid lat/long for the specified location_code
101 |   gps <- csmaps::nor_municip_map_b2024_default_dt[location_code == argset$location_code,.(
102 |     lat = mean(lat),
```

At the top of all `action_fn`s you will again see a section of code wrapped inside `if (plnr::is_run_directly()) {`. This works exactly the same as for the data_selector_fn.

Look at the arguments for `weather_download_and_import_rawdata_data_selector` and you will see that it needs `data`, `argset` and `schema`. The code inside `if (plnr::is_run_directly()) {` loads `data`, `argset` and `schema` for `index_plan = 1` and `index_analysis = 1`. **By running these lines, you can treat the inside of `weather_download_and_import_rawdata_action` as an interactive script!**

  Check out the data, argset and schema you have by running these lines.

#### argset$first_analysis


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L21-L24

21 | 
22 |   # special case that runs before everything
23 |   if (argset$first_analysis == TRUE) {
24 | 
```

This code is only run if it is the first analysis. It is typically used to drop rows in a database, so that the following code may `insert` data (faster) instead of using `upsert` data (slower). If you ran the full task at the beginning of this tutorial you can insert `schema$anon_example_weather_rawdata$drop_all_rows()` inside here to delete the stored data.

#### Doing things

In this tutorial we do not go in to much detail about how the data is collected so for now copy the content of the action function into your action function. (You find it commented out in your file.)



```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L26-L80

26 | 
27 |   a <- data$data$properties$timeseries
28 |   res <- vector("list", length=length(a) - 1)
29 |   for(i in seq_along(res)){
30 |     # i = 1
31 |     time_from <- a[[i]]$time
32 |     if("next_1_hours" %in% names(a[[i]]$data)){
33 |       time_var <- "next_1_hours"
34 |     } else {
35 |       time_var <- "next_6_hours"
36 |     }
37 |     temp <- a[[i]]$data[["instant"]]$details$air_temperature
38 |     precip <- a[[i]]$data[[time_var]]$details$precipitation_amount
39 | 
40 |     res[[i]] <- data.frame(
41 |       time_from = as.character(time_from),
42 |       temp = as.numeric(temp),
43 |       precip = as.numeric(precip)
44 |     )
45 |   }
46 | 
47 |   res <- rbindlist(res)
48 |   res <- res[stringr::str_sub(time_from, 12, 13) %in% c("00", "06", "12", "18")]
49 |   res[, date := as.Date(stringr::str_sub(time_from, 1, 10))]
50 |   res <- res[
51 |     ,
52 |     .(
53 |       temp_max = max(temp),
54 |       temp_min = min(temp),
55 |       precip = sum(precip)
56 |     ),
57 |     keyby = .(date)
58 |   ]
59 | 
60 |   # we look at the downloaded data
61 |   # res
62 | 
63 |   # we now need to format it
64 |   res[, granularity_time := "date"]
65 |   res[, sex := "total"]
66 |   res[, age := "total"]
67 |   res[, location_code := argset$location_code]
68 |   res[, border := global$border]
69 | 
70 |   # fill in missing structural variables
71 |   cstidy::set_csfmt_rts_data_v2(res)
72 | 
73 |   # we look at the downloaded data
74 |   # res
75 | 
76 |   # put data in db table
77 |   tables$anon_example_weather_rawdata$upsert_data(res)
78 | 
79 |   # special case that runs after everything
80 |   if (argset$last_analysis == TRUE) {
```

Every analysis will perform this code.

Run through it line by line and pay special attention to how the data from the data_selector_fn is accesed, the last part where the data is formatted and we use `cstidy::set_csfmt_rts_data_v1(res, border = 2020)` to fill inn the mandatory data columns and the end where the data is inserted to the database.

#### Accessing data from data_selector_fn


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L26-L26

26 | 
```

Here you see that we access the data that was passed to us from `data_selector_fn`

#### Structural data/cstidy::set_csfmt_rts_data_v1


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L68-L74

68 |   res[, border := global$border]
69 | 
70 |   # fill in missing structural variables
71 |   cstidy::set_csfmt_rts_data_v2(res)
72 | 
73 |   # we look at the downloaded data
74 |   # res
```

We have 16 structural data columns that we expect. These columns typically have a lot of redundancy (e.g. date, isoyear, isoyearweek). To make things easier, we provide a function called `cstidy::set_csfmt_rts_data_v1` that uses the information present in the dataset to try and impute the missing structural data.

#### Insert/upsert to databases


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L80-L80

80 |   if (argset$last_analysis == TRUE) {
```

Here we insert the data to the database table.

Insert is an append (so the data cannot already exist in the database table), while upsert is "update (overwrite) if already exists, insert (append) if it doesn't".

If you want to deleate the data use `schema$NAME_DATABASE$drop_all_rows()` in our case `schema$anon_example_weather_rawdata$drop_all_rows()`.

#### argset$last_analysis


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L21-L24

21 | 
22 |   # special case that runs before everything
23 |   if (argset$first_analysis == TRUE) {
24 | 
```

This code is only run if it is the last analysis. It is typically used to copy an internal database table (i.e. one that the public is not directly viewing) to an external database (i.e. one that the public is directly viewing).

By distinguishing between internal database tables (e.g. anon_webkhtint_test) and external database tables (e.g. anon_webkht_test) we can do whatever we want to anon_webkhtint_test while anon_webkht_test remains in place and untouched. This makes it less likely that any mistakes will affect any APIs or websites that the public uses.


### Test the code

The action function should look like this.

```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L1-L86

 1 | # **** action **** ----
 2 | #' weather_download_and_import_rawdata (action)
 3 | #' @param data Data
 4 | #' @param argset Argset
 5 | #' @param tables DB tables
 6 | #' @export
 7 | weather_download_and_import_rawdata_action <- function(data, argset, tables) {
 8 |   # cs9::run_task_sequentially_as_rstudio_job_using_load_all("weather_download_and_import_rawdata")
 9 |   # To be run outside of rstudio: cs9example::global$ss$run_task("weather_download_and_import_rawdata")
10 | 
11 |   if (plnr::is_run_directly()) {
12 |     # global$ss$shortcut_get_plans_argsets_as_dt("weather_download_and_import_rawdata")
13 | 
14 |     index_plan <- 1
15 |     index_analysis <- 1
16 | 
17 |     data <- global$ss$shortcut_get_data("weather_download_and_import_rawdata", index_plan = index_plan)
18 |     argset <- global$ss$shortcut_get_argset("weather_download_and_import_rawdata", index_plan = index_plan, index_analysis = index_analysis)
19 |     tables <- global$ss$shortcut_get_tables("weather_download_and_import_rawdata")
20 |   }
21 | 
22 |   # special case that runs before everything
23 |   if (argset$first_analysis == TRUE) {
24 | 
25 |   }
26 | 
27 |   a <- data$data$properties$timeseries
28 |   res <- vector("list", length=length(a) - 1)
29 |   for(i in seq_along(res)){
30 |     # i = 1
31 |     time_from <- a[[i]]$time
32 |     if("next_1_hours" %in% names(a[[i]]$data)){
33 |       time_var <- "next_1_hours"
34 |     } else {
35 |       time_var <- "next_6_hours"
36 |     }
37 |     temp <- a[[i]]$data[["instant"]]$details$air_temperature
38 |     precip <- a[[i]]$data[[time_var]]$details$precipitation_amount
39 | 
40 |     res[[i]] <- data.frame(
41 |       time_from = as.character(time_from),
42 |       temp = as.numeric(temp),
43 |       precip = as.numeric(precip)
44 |     )
45 |   }
46 | 
47 |   res <- rbindlist(res)
48 |   res <- res[stringr::str_sub(time_from, 12, 13) %in% c("00", "06", "12", "18")]
49 |   res[, date := as.Date(stringr::str_sub(time_from, 1, 10))]
50 |   res <- res[
51 |     ,
52 |     .(
53 |       temp_max = max(temp),
54 |       temp_min = min(temp),
55 |       precip = sum(precip)
56 |     ),
57 |     keyby = .(date)
58 |   ]
59 | 
60 |   # we look at the downloaded data
61 |   # res
62 | 
63 |   # we now need to format it
64 |   res[, granularity_time := "date"]
65 |   res[, sex := "total"]
66 |   res[, age := "total"]
67 |   res[, location_code := argset$location_code]
68 |   res[, border := global$border]
69 | 
70 |   # fill in missing structural variables
71 |   cstidy::set_csfmt_rts_data_v2(res)
72 | 
73 |   # we look at the downloaded data
74 |   # res
75 | 
76 |   # put data in db table
77 |   tables$anon_example_weather_rawdata$upsert_data(res)
78 | 
79 |   # special case that runs after everything
80 |   if (argset$last_analysis == TRUE) {
81 | 
82 |   }
83 | }
84 | 
85 | # **** data_selector **** ----
86 | #' weather_download_and_import_rawdata (data selector)
```

Try and restart, load all and run the code line by line.

### Which plan/analysis is which?

Inside the `if (plnr::is_run_directly()) {` sections, you specify `index_plan` and `index_analysis`. However, these are just numbers. If you want to specifically look at the plan for Oslo municipality, how do you know which `index_plan` this corresponds to?


``` r
options(width = 150)
global$ss$shortcut_get_plans_argsets_as_dt("weather_download_and_import_rawdata")
Error: object 'global' not found
```

Try and change the plan number and run the script again.

Now you have implemented your first task by creating a schema, a task description a data selector function and an action function! Congratulations!

  Run the entire task by running `tm_run_task("weather_download_and_import_rawdata")`.

## Developing weather_clean_data

The previous task (weather_download_and_import_rawdata) focused on downloading raw data from an API and inserting it into a database table.

The task weather_clean_data focuses on cleaning the raw data and inserting it in another database table. That is, the data source is a Sykdomspulsen Core database table, and the output is also a Sykdomspulsen Core database table.

We will walk you through the development of weather_clean_data, however, the description of this task will be less comprehensive than the previous task, and will focus primarily on parts that are novel.

We already mentioned that weather_clean_data cleans the raw data. We want this task to take the raw data we obtained in the previous task and aggregate it to obtain weather data on different geographical regions than municipalities. To do so we use some pre-made FHI functions such as `fhidata::make_skeleton` which makes a data table skeleton for the regions of interest and `fhidata::norway_locations_hierarchy` which converts location codes from one location code level to another.

### 1. Schemas
First we start by creating a schema for the data we want this task to store. The structure is exactly the same as for the previous task with name access anon and temp_max, temp_min and precip as additional colums. Try and create this schema in `03_db_schemas.r`.


```
Warning in file(con, "r"): cannot open URL 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/03_db_schemas.r': HTTP status
was '404 Not Found'
Error in file(con, "r"): cannot open the connection to 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/03_db_schemas.r'
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L66-L113

 66 |   res[, age := "total"]
 67 |   res[, location_code := argset$location_code]
 68 |   res[, border := global$border]
 69 | 
 70 |   # fill in missing structural variables
 71 |   cstidy::set_csfmt_rts_data_v2(res)
 72 | 
 73 |   # we look at the downloaded data
 74 |   # res
 75 | 
 76 |   # put data in db table
 77 |   tables$anon_example_weather_rawdata$upsert_data(res)
 78 | 
 79 |   # special case that runs after everything
 80 |   if (argset$last_analysis == TRUE) {
 81 | 
 82 |   }
 83 | }
 84 | 
 85 | # **** data_selector **** ----
 86 | #' weather_download_and_import_rawdata (data selector)
 87 | #' @param argset Argset
 88 | #' @param tables DB tables
 89 | #' @export
 90 | weather_download_and_import_rawdata_data_selector <- function(argset, tables) {
 91 |   if (plnr::is_run_directly()) {
 92 |     # sc::tm_get_plans_argsets_as_dt("weather_download_and_import_rawdata")
 93 | 
 94 |     index_plan <- 1
 95 | 
 96 |     argset <- global$ss$shortcut_get_argset("weather_download_and_import_rawdata", index_plan = index_plan)
 97 |     tables <- global$ss$shortcut_get_tables("weather_download_and_import_rawdata")
 98 |   }
 99 | 
100 |   # find the mid lat/long for the specified location_code
101 |   gps <- csmaps::nor_municip_map_b2024_default_dt[location_code == argset$location_code,.(
102 |     lat = mean(lat),
103 |     long = mean(long)
104 |   )]
105 | 
106 |   # download the forecast for the specified location_code
107 |   d <- httr::GET(glue::glue("https://api.met.no/weatherapi/locationforecast/2.0/complete?lat={gps$lat}&lon={gps$long}"))
108 |   d <- httr::content(d)
109 | 
110 |   # The variable returned must be a named list
111 |   retval <- list(
112 |     "data" = d
113 |   )
```

### 2. Task definition (task_from_config)

The next step is to define the task in `04_tasks.r` . For this task the aim is to aggregate data to higher levels meaning we need all data available at the same time and we preform the entire task in one analysis. Hence we need a task with only one plan (x = 1). We need the data from the database created in the previous task as input schema and the schema we just implemented as output schema. Try and create this task definition! (Remember to use the addins menu to get a boiler plate task definition.)


#### Plan/analysis structure


```
Warning in file(con, "r"): cannot open URL 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/04_tasks.r': HTTP status was
'404 Not Found'
Error in file(con, "r"): cannot open the connection to 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/04_tasks.r'
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L52-L56

52 |     .(
53 |       temp_max = max(temp),
54 |       temp_min = min(temp),
55 |       precip = sum(precip)
56 |     ),
```

For this particular task, we have decided to only implement one plan containing one analysis, which will process all of the data at once.

If we were only aggregating municipality data to the county level, we could have implemented 11 plans (one for each county). However, because we are also aggregating to the national level, we need all the data available at once.

#### Schemas


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L62-L68

62 | 
63 |   # we now need to format it
64 |   res[, granularity_time := "date"]
65 |   res[, sex := "total"]
66 |   res[, age := "total"]
67 |   res[, location_code := argset$location_code]
68 |   res[, border := global$border]
```

We need to specify the schemas that are used for both input and output.

#### Full task description


```
Warning in file(con, "r"): cannot open URL 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/04_tasks.r': HTTP status was
'404 Not Found'
Error in file(con, "r"): cannot open the connection to 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/04_tasks.r'
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L45-L70

45 |   }
46 | 
47 |   res <- rbindlist(res)
48 |   res <- res[stringr::str_sub(time_from, 12, 13) %in% c("00", "06", "12", "18")]
49 |   res[, date := as.Date(stringr::str_sub(time_from, 1, 10))]
50 |   res <- res[
51 |     ,
52 |     .(
53 |       temp_max = max(temp),
54 |       temp_min = min(temp),
55 |       precip = sum(precip)
56 |     ),
57 |     keyby = .(date)
58 |   ]
59 | 
60 |   # we look at the downloaded data
61 |   # res
62 | 
63 |   # we now need to format it
64 |   res[, granularity_time := "date"]
65 |   res[, sex := "total"]
66 |   res[, age := "total"]
67 |   res[, location_code := argset$location_code]
68 |   res[, border := global$border]
69 | 
70 |   # fill in missing structural variables
```

### 3. data_selector_fn

Now we are ready to create the data selector function. Go to script `weather_clean_data`. Use the addins menu as before to get a boiler plate for the action function and the data selector function and scroll down to the data selector part. Start by inserting your task name instead of `TASK_NAME`.

#### Getting data (specify the schema)

Next fill inn the name of the input schema instead of `SCHEMA_NAME`, connecting to the database table linked to the schema.


```
Warning in file(con, "r"): cannot open URL 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/weather_clean_data.r': HTTP
status was '404 Not Found'
Error in file(con, "r"): cannot open the connection to 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/weather_clean_data.r'
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L200-L200

200 | NA
```


#### Getting data (cs9::mandatory_db_filter)

We then introduce the cs9::mandatory_db_filter. This is a filter on the most common structural variables. We say this is "mandatory" because we want the user to always keep in mind:

  - The minimal amount of data needed to do the job
- To be as explicit as possible with what data is needed to do the job

Fill inn the mandatory filters as best you can and take a peak below if you are not sure.


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L201-L212

201 | NA
202 | NA
203 | NA
204 | NA
205 | NA
206 | NA
207 | NA
208 | NA
209 | NA
210 | NA
211 | NA
212 | NA
```

You will notice that we don't use all of the arguments passed into the function, but we use as many as we can.

#### Getting data (dplyr::select)
We always want to be as explicit as possible with what data is needed to do the job. To achieve this, we use `dplyr::select` to select the columns that we are interested in.

If you want to quickly generate a `dplyr::select` boilerplate for your schema that you can copy/paste, you can do this via either of the following:


``` r
schema$anon_example_weather_rawdata$print_dplyr_select()
```

```
Error: object 'global' not found
```

Use one of these functions and replace the dplyr::select part in your data selector function. To aggregate data we need location_code, date, temp_max, temp_min, precip, and the granularity_time (dayly, weekly, etc). Comment out the other variables.



```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L213-L237

213 | NA
214 | NA
215 | NA
216 | NA
217 | NA
218 | NA
219 | NA
220 | NA
221 | NA
222 | NA
223 | NA
224 | NA
225 | NA
226 | NA
227 | NA
228 | NA
229 | NA
230 | NA
231 | NA
232 | NA
233 | NA
234 | NA
235 | NA
236 | NA
237 | NA
```


#### Getting data (dplyr::collect)


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L238-L238

238 | NA
```

This executes the SQL call to the database.

#### Getting data (data.table and setorder)


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L239-L243

239 | NA
240 | NA
241 | NA
242 | NA
243 | NA
```

Firstly, as a general rule we prefer to use data.table. So we would like to convert our data.frame to a data.table.

Secondly, we are not guaranteed to receive our data in any particular order. Because of this, it is very important that we sort our data on arrival (if this is relevant to the action_fn, e.g. if cumulative sums are created).

#### Set a name

Finally give the dataset you return a suitable name for example `day_municip`.

Check that you data selector function works by saving, restarting, and loading all packages. Then run through the function line by line.

#### Example of the data_selector function

```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L184-L251

184 | NA
185 | NA
186 | NA
187 | NA
188 | NA
189 | NA
190 | NA
191 | NA
192 | NA
193 | NA
194 | NA
195 | NA
196 | NA
197 | NA
198 | NA
199 | NA
200 | NA
201 | NA
202 | NA
203 | NA
204 | NA
205 | NA
206 | NA
207 | NA
208 | NA
209 | NA
210 | NA
211 | NA
212 | NA
213 | NA
214 | NA
215 | NA
216 | NA
217 | NA
218 | NA
219 | NA
220 | NA
221 | NA
222 | NA
223 | NA
224 | NA
225 | NA
226 | NA
227 | NA
228 | NA
229 | NA
230 | NA
231 | NA
232 | NA
233 | NA
234 | NA
235 | NA
236 | NA
237 | NA
238 | NA
239 | NA
240 | NA
241 | NA
242 | NA
243 | NA
244 | NA
245 | NA
246 | NA
247 | NA
248 | NA
249 | NA
250 | NA
251 | NA
```

### 4. action_fn

The final step in the process is creating the action function. Replace `TASK_NAME` with your task name.

#### Skeleton
In this action function we use fhi skeletons to create bases for our data tables.
Read [here](https://folkehelseinstituttet.github.io/fhidata/articles/Skeletons.html) about the concept of skeletons.

Start by creating a variable (for example d_agg) for an empty list and copy the data collected in the data selector function into this (`d_agg$day_municip <- copy(data$day_municip)`). Extract the first and last date from this dataset.

Now we are going to create a skeleton for where we separate between regions where we have data (municipalities) and regions where we do not have data (bo og arbeigs regioner). The skeleton function takes min and max dates and in this case we will pass it granularity_geo consisting of a list with



``` r
list(
          "nodata" = c(
          "wardoslo",
          "extrawardoslo",
          "missingwardoslo",
          "wardbergen",
          "missingwardbergen",
          "wardstavanger",
          "missingwardstavanger",
          "notmainlandmunicip",
          "missingmunicip",
          "notmainlandcounty",
          "missingcounty"
        ),
        "municip" = c(
          "municip"
        )
)
$nodata
 [1] "wardoslo"             "extrawardoslo"        "missingwardoslo"      "wardbergen"           "missingwardbergen"    "wardstavanger"       
 [7] "missingwardstavanger" "notmainlandmunicip"   "missingmunicip"       "notmainlandcounty"    "missingcounty"       

$municip
[1] "municip"
```

#### Merge in weather data

Next we want to merge the information we have on weather data for the municipalities into this data.



```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L67-L84

67 |   res[, location_code := argset$location_code]
68 |   res[, border := global$border]
69 | 
70 |   # fill in missing structural variables
71 |   cstidy::set_csfmt_rts_data_v2(res)
72 | 
73 |   # we look at the downloaded data
74 |   # res
75 | 
76 |   # put data in db table
77 |   tables$anon_example_weather_rawdata$upsert_data(res)
78 | 
79 |   # special case that runs after everything
80 |   if (argset$last_analysis == TRUE) {
81 | 
82 |   }
83 | }
84 | 
```

#### Aggregate to a county level

Now aggregate the data to a county level with the help of `fhidata::norway_locations_hierarchy`


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L86-L109

 86 | #' weather_download_and_import_rawdata (data selector)
 87 | #' @param argset Argset
 88 | #' @param tables DB tables
 89 | #' @export
 90 | weather_download_and_import_rawdata_data_selector <- function(argset, tables) {
 91 |   if (plnr::is_run_directly()) {
 92 |     # sc::tm_get_plans_argsets_as_dt("weather_download_and_import_rawdata")
 93 | 
 94 |     index_plan <- 1
 95 | 
 96 |     argset <- global$ss$shortcut_get_argset("weather_download_and_import_rawdata", index_plan = index_plan)
 97 |     tables <- global$ss$shortcut_get_tables("weather_download_and_import_rawdata")
 98 |   }
 99 | 
100 |   # find the mid lat/long for the specified location_code
101 |   gps <- csmaps::nor_municip_map_b2024_default_dt[location_code == argset$location_code,.(
102 |     lat = mean(lat),
103 |     long = mean(long)
104 |   )]
105 | 
106 |   # download the forecast for the specified location_code
107 |   d <- httr::GET(glue::glue("https://api.met.no/weatherapi/locationforecast/2.0/complete?lat={gps$lat}&lon={gps$long}"))
108 |   d <- httr::content(d)
109 | 
```


#### Aggregate to national level

There is no overlap in municipalities, hence aggregating to a national level can be done without the help of `fhidata::norway_locations_hierarchy`.


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L111-L127

111 |   retval <- list(
112 |     "data" = d
113 |   )
114 | 
115 |   retval
116 | }
117 | 
118 | # **** functions **** ----
119 | NA
120 | NA
121 | NA
122 | NA
123 | NA
124 | NA
125 | NA
126 | NA
127 | NA
```

#### Combine data

Combine all the different granularity geos by using rbindlist and storing it to a new name f.eks skeleton_day.



#### Weekly data.

As a challenge try and aggregate the daily data to weekly data! You can use `lubridate::isoyear()` and `lubridate::isoweek()` functions to get the ISO year and week of a date.

```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L134-L153

134 | NA
135 | NA
136 | NA
137 | NA
138 | NA
139 | NA
140 | NA
141 | NA
142 | NA
143 | NA
144 | NA
145 | NA
146 | NA
147 | NA
148 | NA
149 | NA
150 | NA
151 | NA
152 | NA
153 | NA
```

#### Structural data

The next step is to fill in all missing structural data. Fill in sex = "total" and age= "total" manually then you can use `cstidy::set_csfmt_rts_data_v1(skeleton_day, border = config$border)`.
For the weekly data make sure to also convert the date by using `as.Date(date)` to ensure it is on the right format.

Rbindlist binds the two data tables together.


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L155-L173

155 | NA
156 | NA
157 | NA
158 | NA
159 | NA
160 | NA
161 | NA
162 | NA
163 | NA
164 | NA
165 | NA
166 | NA
167 | NA
168 | NA
169 | NA
170 | NA
171 | NA
172 | NA
173 | NA
```

#### Store the data

Insert the final data table into the database specified in the task description


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L175-L176

175 | NA
176 | NA
```


Restart R, load all packages and try and run the task.

Run the entire task by running `tm_run_task("weather_clean_data")`. If you ran the tasks at the beginning of the script you might need to run `schema$anon_example_weather_data$drop_all_rows()` first.

#### Full example

```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L1-L182

  1 | # **** action **** ----
  2 | #' weather_download_and_import_rawdata (action)
  3 | #' @param data Data
  4 | #' @param argset Argset
  5 | #' @param tables DB tables
  6 | #' @export
  7 | weather_download_and_import_rawdata_action <- function(data, argset, tables) {
  8 |   # cs9::run_task_sequentially_as_rstudio_job_using_load_all("weather_download_and_import_rawdata")
  9 |   # To be run outside of rstudio: cs9example::global$ss$run_task("weather_download_and_import_rawdata")
 10 | 
 11 |   if (plnr::is_run_directly()) {
 12 |     # global$ss$shortcut_get_plans_argsets_as_dt("weather_download_and_import_rawdata")
 13 | 
 14 |     index_plan <- 1
 15 |     index_analysis <- 1
 16 | 
 17 |     data <- global$ss$shortcut_get_data("weather_download_and_import_rawdata", index_plan = index_plan)
 18 |     argset <- global$ss$shortcut_get_argset("weather_download_and_import_rawdata", index_plan = index_plan, index_analysis = index_analysis)
 19 |     tables <- global$ss$shortcut_get_tables("weather_download_and_import_rawdata")
 20 |   }
 21 | 
 22 |   # special case that runs before everything
 23 |   if (argset$first_analysis == TRUE) {
 24 | 
 25 |   }
 26 | 
 27 |   a <- data$data$properties$timeseries
 28 |   res <- vector("list", length=length(a) - 1)
 29 |   for(i in seq_along(res)){
 30 |     # i = 1
 31 |     time_from <- a[[i]]$time
 32 |     if("next_1_hours" %in% names(a[[i]]$data)){
 33 |       time_var <- "next_1_hours"
 34 |     } else {
 35 |       time_var <- "next_6_hours"
 36 |     }
 37 |     temp <- a[[i]]$data[["instant"]]$details$air_temperature
 38 |     precip <- a[[i]]$data[[time_var]]$details$precipitation_amount
 39 | 
 40 |     res[[i]] <- data.frame(
 41 |       time_from = as.character(time_from),
 42 |       temp = as.numeric(temp),
 43 |       precip = as.numeric(precip)
 44 |     )
 45 |   }
 46 | 
 47 |   res <- rbindlist(res)
 48 |   res <- res[stringr::str_sub(time_from, 12, 13) %in% c("00", "06", "12", "18")]
 49 |   res[, date := as.Date(stringr::str_sub(time_from, 1, 10))]
 50 |   res <- res[
 51 |     ,
 52 |     .(
 53 |       temp_max = max(temp),
 54 |       temp_min = min(temp),
 55 |       precip = sum(precip)
 56 |     ),
 57 |     keyby = .(date)
 58 |   ]
 59 | 
 60 |   # we look at the downloaded data
 61 |   # res
 62 | 
 63 |   # we now need to format it
 64 |   res[, granularity_time := "date"]
 65 |   res[, sex := "total"]
 66 |   res[, age := "total"]
 67 |   res[, location_code := argset$location_code]
 68 |   res[, border := global$border]
 69 | 
 70 |   # fill in missing structural variables
 71 |   cstidy::set_csfmt_rts_data_v2(res)
 72 | 
 73 |   # we look at the downloaded data
 74 |   # res
 75 | 
 76 |   # put data in db table
 77 |   tables$anon_example_weather_rawdata$upsert_data(res)
 78 | 
 79 |   # special case that runs after everything
 80 |   if (argset$last_analysis == TRUE) {
 81 | 
 82 |   }
 83 | }
 84 | 
 85 | # **** data_selector **** ----
 86 | #' weather_download_and_import_rawdata (data selector)
 87 | #' @param argset Argset
 88 | #' @param tables DB tables
 89 | #' @export
 90 | weather_download_and_import_rawdata_data_selector <- function(argset, tables) {
 91 |   if (plnr::is_run_directly()) {
 92 |     # sc::tm_get_plans_argsets_as_dt("weather_download_and_import_rawdata")
 93 | 
 94 |     index_plan <- 1
 95 | 
 96 |     argset <- global$ss$shortcut_get_argset("weather_download_and_import_rawdata", index_plan = index_plan)
 97 |     tables <- global$ss$shortcut_get_tables("weather_download_and_import_rawdata")
 98 |   }
 99 | 
100 |   # find the mid lat/long for the specified location_code
101 |   gps <- csmaps::nor_municip_map_b2024_default_dt[location_code == argset$location_code,.(
102 |     lat = mean(lat),
103 |     long = mean(long)
104 |   )]
105 | 
106 |   # download the forecast for the specified location_code
107 |   d <- httr::GET(glue::glue("https://api.met.no/weatherapi/locationforecast/2.0/complete?lat={gps$lat}&lon={gps$long}"))
108 |   d <- httr::content(d)
109 | 
110 |   # The variable returned must be a named list
111 |   retval <- list(
112 |     "data" = d
113 |   )
114 | 
115 |   retval
116 | }
117 | 
118 | # **** functions **** ----
119 | NA
120 | NA
121 | NA
122 | NA
123 | NA
124 | NA
125 | NA
126 | NA
127 | NA
128 | NA
129 | NA
130 | NA
131 | NA
132 | NA
133 | NA
134 | NA
135 | NA
136 | NA
137 | NA
138 | NA
139 | NA
140 | NA
141 | NA
142 | NA
143 | NA
144 | NA
145 | NA
146 | NA
147 | NA
148 | NA
149 | NA
150 | NA
151 | NA
152 | NA
153 | NA
154 | NA
155 | NA
156 | NA
157 | NA
158 | NA
159 | NA
160 | NA
161 | NA
162 | NA
163 | NA
164 | NA
165 | NA
166 | NA
167 | NA
168 | NA
169 | NA
170 | NA
171 | NA
172 | NA
173 | NA
174 | NA
175 | NA
176 | NA
177 | NA
178 | NA
179 | NA
180 | NA
181 | NA
182 | NA
```


## Developing weather_export_plots

The final task of this tutorial, weather_export_plots, takes the cleaned data and plots 11 graphs (one for each county) of min and max temperatures.
This means we need 11 plans, one for each county. We use input data generated by data clean_weather_data. Hence, we do not need to create a new schema. We are going to pass a few universal argset through the task definition to define the location to store the figures.



```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L84-L88

84 | 
85 | # **** data_selector **** ----
86 | #' weather_download_and_import_rawdata (data selector)
87 | #' @param argset Argset
88 | #' @param tables DB tables
```

The benefits of placing the output directories and filenames in the task declaration are:

- It makes your action_fn more generic, and can be reused by multiple tasks
- It is easier to get an overview of where the output is being sent
- "More decisions" in the task config and "fewer decisions" in the action_fn makes the system easier for everyone to understand, because decisions become more explicit

Everything inside the curly brackets get passed through the action function.

Each plan only need the data for that specific location_code. This can be implemented in the manditory filters in the data selector function.

`fs::dir_create(glue::glue(argset$output_dir))` can be used to create the output directory.

Try putting everything you have learned so fare together and create this task by yourself. If you get stuck you can always peak below. Good luck!

### 1. Schemas


```
Warning in file(con, "r"): cannot open URL 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/03_db_schemas.r': HTTP status
was '404 Not Found'
Error in file(con, "r"): cannot open the connection to 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/03_db_schemas.r'
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L66-L112

 66 |   res[, age := "total"]
 67 |   res[, location_code := argset$location_code]
 68 |   res[, border := global$border]
 69 | 
 70 |   # fill in missing structural variables
 71 |   cstidy::set_csfmt_rts_data_v2(res)
 72 | 
 73 |   # we look at the downloaded data
 74 |   # res
 75 | 
 76 |   # put data in db table
 77 |   tables$anon_example_weather_rawdata$upsert_data(res)
 78 | 
 79 |   # special case that runs after everything
 80 |   if (argset$last_analysis == TRUE) {
 81 | 
 82 |   }
 83 | }
 84 | 
 85 | # **** data_selector **** ----
 86 | #' weather_download_and_import_rawdata (data selector)
 87 | #' @param argset Argset
 88 | #' @param tables DB tables
 89 | #' @export
 90 | weather_download_and_import_rawdata_data_selector <- function(argset, tables) {
 91 |   if (plnr::is_run_directly()) {
 92 |     # sc::tm_get_plans_argsets_as_dt("weather_download_and_import_rawdata")
 93 | 
 94 |     index_plan <- 1
 95 | 
 96 |     argset <- global$ss$shortcut_get_argset("weather_download_and_import_rawdata", index_plan = index_plan)
 97 |     tables <- global$ss$shortcut_get_tables("weather_download_and_import_rawdata")
 98 |   }
 99 | 
100 |   # find the mid lat/long for the specified location_code
101 |   gps <- csmaps::nor_municip_map_b2024_default_dt[location_code == argset$location_code,.(
102 |     lat = mean(lat),
103 |     long = mean(long)
104 |   )]
105 | 
106 |   # download the forecast for the specified location_code
107 |   d <- httr::GET(glue::glue("https://api.met.no/weatherapi/locationforecast/2.0/complete?lat={gps$lat}&lon={gps$long}"))
108 |   d <- httr::content(d)
109 | 
110 |   # The variable returned must be a named list
111 |   retval <- list(
112 |     "data" = d
```

This schema has already been created by the previous task `weather_clean_data`.

### 2. Task definition (task_from_config)


```
Warning in file(con, "r"): cannot open URL 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/04_tasks.r': HTTP status was
'404 Not Found'
Error in file(con, "r"): cannot open the connection to 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/04_tasks.r'
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L72-L100

 72 | 
 73 |   # we look at the downloaded data
 74 |   # res
 75 | 
 76 |   # put data in db table
 77 |   tables$anon_example_weather_rawdata$upsert_data(res)
 78 | 
 79 |   # special case that runs after everything
 80 |   if (argset$last_analysis == TRUE) {
 81 | 
 82 |   }
 83 | }
 84 | 
 85 | # **** data_selector **** ----
 86 | #' weather_download_and_import_rawdata (data selector)
 87 | #' @param argset Argset
 88 | #' @param tables DB tables
 89 | #' @export
 90 | weather_download_and_import_rawdata_data_selector <- function(argset, tables) {
 91 |   if (plnr::is_run_directly()) {
 92 |     # sc::tm_get_plans_argsets_as_dt("weather_download_and_import_rawdata")
 93 | 
 94 |     index_plan <- 1
 95 | 
 96 |     argset <- global$ss$shortcut_get_argset("weather_download_and_import_rawdata", index_plan = index_plan)
 97 |     tables <- global$ss$shortcut_get_tables("weather_download_and_import_rawdata")
 98 |   }
 99 | 
100 |   # find the mid lat/long for the specified location_code
```

#### Plan/analysis structure


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L79-L83

79 |   # special case that runs after everything
80 |   if (argset$last_analysis == TRUE) {
81 | 
82 |   }
83 | }
```

Here we choose a plan-heavy approach (11 plans, 1 analysis per plan) to minimize the amount of data loaded into RAM at any point in time.

#### Universal argset


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L84-L88

84 | 
85 | # **** data_selector **** ----
86 | #' weather_download_and_import_rawdata (data selector)
87 | #' @param argset Argset
88 | #' @param tables DB tables
```


### 3. data_selector_fn


```
Warning in file(con, "r"): cannot open URL 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/weather_export_plots.r': HTTP
status was '404 Not Found'
Error in file(con, "r"): cannot open the connection to 'https://raw.githubusercontent.com/sykdomspulsen-org/sc-tutorial-end/main/R/weather_export_plots.r'
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L45-L110

 45 |   }
 46 | 
 47 |   res <- rbindlist(res)
 48 |   res <- res[stringr::str_sub(time_from, 12, 13) %in% c("00", "06", "12", "18")]
 49 |   res[, date := as.Date(stringr::str_sub(time_from, 1, 10))]
 50 |   res <- res[
 51 |     ,
 52 |     .(
 53 |       temp_max = max(temp),
 54 |       temp_min = min(temp),
 55 |       precip = sum(precip)
 56 |     ),
 57 |     keyby = .(date)
 58 |   ]
 59 | 
 60 |   # we look at the downloaded data
 61 |   # res
 62 | 
 63 |   # we now need to format it
 64 |   res[, granularity_time := "date"]
 65 |   res[, sex := "total"]
 66 |   res[, age := "total"]
 67 |   res[, location_code := argset$location_code]
 68 |   res[, border := global$border]
 69 | 
 70 |   # fill in missing structural variables
 71 |   cstidy::set_csfmt_rts_data_v2(res)
 72 | 
 73 |   # we look at the downloaded data
 74 |   # res
 75 | 
 76 |   # put data in db table
 77 |   tables$anon_example_weather_rawdata$upsert_data(res)
 78 | 
 79 |   # special case that runs after everything
 80 |   if (argset$last_analysis == TRUE) {
 81 | 
 82 |   }
 83 | }
 84 | 
 85 | # **** data_selector **** ----
 86 | #' weather_download_and_import_rawdata (data selector)
 87 | #' @param argset Argset
 88 | #' @param tables DB tables
 89 | #' @export
 90 | weather_download_and_import_rawdata_data_selector <- function(argset, tables) {
 91 |   if (plnr::is_run_directly()) {
 92 |     # sc::tm_get_plans_argsets_as_dt("weather_download_and_import_rawdata")
 93 | 
 94 |     index_plan <- 1
 95 | 
 96 |     argset <- global$ss$shortcut_get_argset("weather_download_and_import_rawdata", index_plan = index_plan)
 97 |     tables <- global$ss$shortcut_get_tables("weather_download_and_import_rawdata")
 98 |   }
 99 | 
100 |   # find the mid lat/long for the specified location_code
101 |   gps <- csmaps::nor_municip_map_b2024_default_dt[location_code == argset$location_code,.(
102 |     lat = mean(lat),
103 |     long = mean(long)
104 |   )]
105 | 
106 |   # download the forecast for the specified location_code
107 |   d <- httr::GET(glue::glue("https://api.met.no/weatherapi/locationforecast/2.0/complete?lat={gps$lat}&lon={gps$long}"))
108 |   d <- httr::content(d)
109 | 
110 |   # The variable returned must be a named list
```

### 4. action_fn


```
https://github.com/csids/cs9example/blob/main/R/weather_download_and_import_rawdata.R#L1-L43

 1 | # **** action **** ----
 2 | #' weather_download_and_import_rawdata (action)
 3 | #' @param data Data
 4 | #' @param argset Argset
 5 | #' @param tables DB tables
 6 | #' @export
 7 | weather_download_and_import_rawdata_action <- function(data, argset, tables) {
 8 |   # cs9::run_task_sequentially_as_rstudio_job_using_load_all("weather_download_and_import_rawdata")
 9 |   # To be run outside of rstudio: cs9example::global$ss$run_task("weather_download_and_import_rawdata")
10 | 
11 |   if (plnr::is_run_directly()) {
12 |     # global$ss$shortcut_get_plans_argsets_as_dt("weather_download_and_import_rawdata")
13 | 
14 |     index_plan <- 1
15 |     index_analysis <- 1
16 | 
17 |     data <- global$ss$shortcut_get_data("weather_download_and_import_rawdata", index_plan = index_plan)
18 |     argset <- global$ss$shortcut_get_argset("weather_download_and_import_rawdata", index_plan = index_plan, index_analysis = index_analysis)
19 |     tables <- global$ss$shortcut_get_tables("weather_download_and_import_rawdata")
20 |   }
21 | 
22 |   # special case that runs before everything
23 |   if (argset$first_analysis == TRUE) {
24 | 
25 |   }
26 | 
27 |   a <- data$data$properties$timeseries
28 |   res <- vector("list", length=length(a) - 1)
29 |   for(i in seq_along(res)){
30 |     # i = 1
31 |     time_from <- a[[i]]$time
32 |     if("next_1_hours" %in% names(a[[i]]$data)){
33 |       time_var <- "next_1_hours"
34 |     } else {
35 |       time_var <- "next_6_hours"
36 |     }
37 |     temp <- a[[i]]$data[["instant"]]$details$air_temperature
38 |     precip <- a[[i]]$data[[time_var]]$details$precipitation_amount
39 | 
40 |     res[[i]] <- data.frame(
41 |       time_from = as.character(time_from),
42 |       temp = as.numeric(temp),
43 |       precip = as.numeric(precip)
```

## Final package

If you save, restart and load all all packages you can now see which schemas have been loaded by running `names(global$ss$tables)`. You can now see that the schemas you made are included.


``` r
names(global$ss$tables)
Error: object 'global' not found
```


You can alsp see which tasks have been loaded by running `names(global$ss$tasks)`. These tasks are included in the skeleton.


``` r
names(global$ss$tasks)
Error: object 'global' not found
```

### Running

You can now run these tasks in your console if you want. Note that we use `scskeleton::tm_run_task` instead of `global$ss$run_task`. This is because we want to ensure that `scexample::.onLoad` has been called which authenticates you.


``` r
scskeleton::tm_run_task("weather_download_and_import_rawdata")
scskeleton::tm_run_task("weather_clean_data")
scskeleton::tm_run_task("weather_export_weather_plots")
```


Congratulations! You have now successfully finnished your first tutorial on Sykdomspulsen Core.

## What now?

After Tutorial 1, we expect that you understand the four fundamental parts of developing a task:

1. Schemas
2. Task definition (task_from_config)
3. data_selector_fn
4. action_fn

We also expect that you can:

1. Run a task using `tm_run_task`
2. Use `global$ss$shortcut_get_plans_argsets_as_dt` to identify which `index_plan` and `index_analysis` corresponds to the plan/analysis you are interested in (e.g. Oslo)
2. Run the inside code of a `data_selector_fn` for different `index_plan`s as if it were an interactive script
3. Run the inside code of an `action_fn` for different `index_plan`s and `index_analysis`s as if it were an interactive script

<!-- Tutorial 2 will challenge you to start creating your own tasks to solve problems. -->
